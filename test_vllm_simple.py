#!/usr/bin/env python3
"""
ç®€å•çš„vLLMæµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯vLLMæ˜¯å¦èƒ½æ­£å¸¸åŠ è½½å’Œè¿è¡Œ
"""

import os
import time
from vllm import LLM, SamplingParams


def main():
    print("ğŸš€ å¼€å§‹vLLMç®€å•æµ‹è¯•...")
    
    try:
        # æ¨¡å‹è·¯å¾„
        model_path = "./nanovllm/qwen3_0.6b/"
        print(f"ğŸ“‚ åŠ è½½æ¨¡å‹: {model_path}")
        
        # åˆ›å»ºLLMå®ä¾‹
        llm = LLM(
            model=model_path,
            enforce_eager=False,
            max_model_len=4096,
            tensor_parallel_size=2
        )
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # ç®€å•çš„æ¨ç†æµ‹è¯•
        prompts = ["Hello, how are you?"]
        sampling_params = SamplingParams(
            temperature=0.6,
            max_tokens=50
        )
        
        print("ğŸ”„ å¼€å§‹æ¨ç†æµ‹è¯•...")
        start_time = time.time()
        
        outputs = llm.generate(prompts, sampling_params)
        
        end_time = time.time()
        
        print("âœ… æ¨ç†å®Œæˆ")
        print(f"â±ï¸  æ¨ç†æ—¶é—´: {end_time - start_time:.2f}ç§’")
        
        # æ˜¾ç¤ºç»“æœ
        for output in outputs:
            prompt = output.prompt
            generated_text = output.outputs[0].text
            print(f"\nğŸ“ è¾“å…¥: {prompt}")
            print(f"ğŸ“¤ è¾“å‡º: {generated_text}")
        
        print("\nğŸ‰ vLLMæµ‹è¯•æˆåŠŸå®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ vLLMæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)